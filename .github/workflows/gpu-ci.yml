name: GPU CI

on:
  pull_request:
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - 'LICENSE'
      - '.gitignore'

jobs:
  gpu-tests:
    runs-on: self-hosted
    timeout-minutes: 150
    
    steps:
    - name: Clean workspace
      run: |
        echo "Cleaning workspace to avoid stale artifacts..."
        rm -rf ${GITHUB_WORKSPACE}/build_*
        rm -rf ${GITHUB_WORKSPACE}/CMakeCache.txt
        rm -rf ${GITHUB_WORKSPACE}/CMakeFiles
        echo "Workspace cleaned"
    
    - uses: actions/checkout@v3
    
    - name: Submit GPU correctness suite to Slurm (H200, 1 GPU)
      run: |
        set -euo pipefail
        
        SBATCH_SCRIPT="${GITHUB_WORKSPACE}/gpu_ci_correctness_${GITHUB_RUN_ID}.sbatch"
        SLURM_OUT="${GITHUB_WORKSPACE}/gpu_ci_correctness_${GITHUB_RUN_ID}.out"
        SLURM_ERR="${GITHUB_WORKSPACE}/gpu_ci_correctness_${GITHUB_RUN_ID}.err"
        
        cat > "${SBATCH_SCRIPT}" <<'EOF'
        #!/bin/bash
        #SBATCH -J gpu-ci-correctness
        #SBATCH -A gts-sbryngelson3
        #SBATCH -N 1
        #SBATCH --ntasks-per-node=4
        #SBATCH -p gpu-h200
        #SBATCH -G 1
        #SBATCH --qos=embers
        #SBATCH -t 01:55:00
        #SBATCH -o __SLURM_OUT__
        #SBATCH -e __SLURM_ERR__
        
        set -euo pipefail
        
        module reset
        module load nvhpc/25.5
        
        echo "==================================================================="
        echo "  GPU CI Slurm Job Environment"
        echo "==================================================================="
        echo ""
        echo "Host: $(hostname)"
        echo "Date: $(date)"
        echo ""
        echo "GPU(s):"
        nvidia-smi
        echo ""
        
        # Hard-require OpenMP target offload (fail if it falls back to CPU)
        export OMP_TARGET_OFFLOAD=MANDATORY
        
        cd "__WORKDIR__"
        
        chmod +x scripts/run_gpu_ci_test.sh
        chmod +x .github/scripts/*.sh
    
        # Clean rebuild (correctness suite must rebuild first)
        rm -rf build_ci_gpu_correctness
        mkdir -p build_ci_gpu_correctness
        cd build_ci_gpu_correctness
        
        echo "=== CMake Configuration ==="
        CC=nvc CXX=nvc++ cmake .. -DCMAKE_BUILD_TYPE=Release -DUSE_GPU_OFFLOAD=ON 2>&1 | tee cmake_config.log
        echo ""
        echo "=== CMake Configuration Summary ==="
        grep -E "GPU offload|CXX compiler|OpenMP|NVIDIA" cmake_config.log || echo "No GPU-related config found"
        echo ""
        echo "=== Building ==="
        make -j8
        mkdir -p output

        echo ""
        echo "==================================================================="
        echo "  CPU-only build vs GPU-offload build (perturbed Poisson compare)"
        echo "==================================================================="
        echo ""

        # Build a CPU-only reference binary (no GPU offload) and run the same case.
        cd "__WORKDIR__"
        rm -rf build_ci_cpu_ref
        mkdir -p build_ci_cpu_ref
        cd build_ci_cpu_ref

        echo "=== CMake Configuration (CPU-only reference) ==="
        CC=nvc CXX=nvc++ cmake .. -DCMAKE_BUILD_TYPE=Release -DUSE_GPU_OFFLOAD=OFF 2>&1 | tee cmake_config.log
        echo ""
        echo "=== Building (CPU-only reference) ==="
        make -j8
        mkdir -p output/cpu_ref

        echo ""
        echo "--- Running perturbed Poisson test (CPU-only build) ---"
        ./test_poisson_perturbed --Nx 64 --Ny 64 --nu 0.01 --dp_dx -0.01 --max_iter 2000 --tol 1e-6 --verbose false \
          | tee output/cpu_ref/poisson_perturbed.log

        # Now run the same case with the GPU-offload build (must offload due to MANDATORY).
        cd "__WORKDIR__/build_ci_gpu_correctness"
        mkdir -p output/gpu_ref

        echo ""
        echo "--- Running perturbed Poisson test (GPU-offload build) ---"
        ./test_poisson_perturbed --Nx 64 --Ny 64 --nu 0.01 --dp_dx -0.01 --max_iter 2000 --tol 1e-6 --verbose false \
          | tee output/gpu_ref/poisson_perturbed.log

        echo ""
        echo "--- Comparing perturbed Poisson diagnostics: CPU-only vs GPU-offload build ---"
        python3 - <<'PY'
        import math
        import os
        import re
        import sys

        def parse_metrics(log_path: str):
            if not os.path.exists(log_path):
                raise FileNotFoundError(log_path)
            txt = open(log_path, "r", encoding="utf-8", errors="ignore").read()

            def grab(key: str, pat: str):
                m = re.search(pat, txt)
                if not m:
                    raise RuntimeError(f"missing '{key}' in {log_path}")
                return float(m.group(1))

            metrics = {
                "residual": grab("residual", r"Final residual:\\s*([+-]?(?:\\d+\\.\\d*|\\d*\\.\\d+|\\d+)(?:[eE][+-]?\\d+)?)"),
                "iterations": grab("iterations", r"Iterations:\\s*([0-9]+)"),
                "bulk_velocity": grab("bulk_velocity", r"Bulk velocity:\\s*([+-]?(?:\\d+\\.\\d*|\\d*\\.\\d+|\\d+)(?:[eE][+-]?\\d+)?)"),
                "max_div": grab("max_div", r"Max divergence:\\s*([+-]?(?:\\d+\\.\\d*|\\d*\\.\\d+|\\d+)(?:[eE][+-]?\\d+)?)"),
                "rms_div": grab("rms_div", r"RMS divergence:\\s*([+-]?(?:\\d+\\.\\d*|\\d*\\.\\d+|\\d+)(?:[eE][+-]?\\d+)?)"),
            }
            return metrics

        def cmp(name: str, a: float, b: float, tol: float):
            d = abs(a - b)
            ok = d <= tol
            print(f"{name}: cpu={a:.12e} gpu={b:.12e} diff={d:.3e} tol={tol:.1e} {'OK' if ok else 'FAIL'}")
            return ok

        cpu_log = "__WORKDIR__/build_ci_cpu_ref/output/cpu_ref/poisson_perturbed.log"
        gpu_log = "__WORKDIR__/build_ci_gpu_correctness/output/gpu_ref/poisson_perturbed.log"

        cpu = parse_metrics(cpu_log)
        gpu = parse_metrics(gpu_log)

        print("Parsed metrics (CPU-only):", cpu)
        print("Parsed metrics (GPU-offload):", gpu)

        # Iteration counts can legitimately differ across builds; report only.
        print(f"iterations: cpu={int(cpu['iterations'])} gpu={int(gpu['iterations'])} (not enforced)")

        ok = True
        # These should match closely if CPU/GPU paths are consistent.
        ok &= cmp("bulk_velocity", cpu["bulk_velocity"], gpu["bulk_velocity"], tol=1e-6)
        ok &= cmp("max_div", cpu["max_div"], gpu["max_div"], tol=1e-8)
        ok &= cmp("rms_div", cpu["rms_div"], gpu["rms_div"], tol=1e-8)
        # Residual can vary depending on iteration path; keep looser but still sanity-check.
        ok &= cmp("residual", cpu["residual"], gpu["residual"], tol=1e-5)

        sys.exit(0 if ok else 1)
        PY

        echo ""
        echo "==================================================================="
        echo "  End-to-end lockstep CPU vs GPU (single binary)"
        echo "==================================================================="
        echo ""
        # Single binary runs CPU-forced then GPU turbulence and compares final fields.
        ./compare_channel_cpu_gpu --Nx 64 --Ny 128 --max_iter 200 --model baseline
    
        echo ""
        echo "==================================================================="
        echo "  Comprehensive GPU tests"
        echo "==================================================================="
        echo ""
        
        ../scripts/run_gpu_ci_test.sh "all_gpu_tests" \
          "bash -c '\
            chmod +x ../.github/scripts/*.sh && \
            mkdir -p output/gpu_validation && \
            echo \"======================================\" && \
            echo \"1. Unit Tests\" && \
            echo \"======================================\" && \
            ctest --output-on-failure && \
            echo \"\" && \
            echo \"======================================\" && \
            echo \"2. Algebraic Models (Fast Validation)\" && \
            echo \"======================================\" && \
            ../.github/scripts/test_turbulence_model_gpu.sh baseline \"Baseline\" 64 128 5000 output/gpu_validation/baseline && \
            ../.github/scripts/test_turbulence_model_gpu.sh gep \"GEP\" 64 128 5000 output/gpu_validation/gep && \
            echo \"\" && \
            echo \"======================================\" && \
            echo \"3. Neural Network Models\" && \
            echo \"======================================\" && \
            echo \"   3a. NN-MLP (skipping - requires model files)\" && \
            echo \"   3b. NN-TBNN (skipping - requires model files)\" && \
            echo \"\" && \
            echo \"======================================\" && \
            echo \"4. Transport Equation Models (Fast Validation)\" && \
            echo \"======================================\" && \
            ../.github/scripts/test_turbulence_model_gpu.sh sst \"SST k-omega\" 64 128 500 output/gpu_validation/sst && \
            ../.github/scripts/test_turbulence_model_gpu.sh komega \"k-omega (Wilcox)\" 64 128 500 output/gpu_validation/komega && \
            echo \"\" && \
            echo \"======================================\" && \
            echo \"5. EARSM Models\" && \
            echo \"======================================\" && \
            ../.github/scripts/test_turbulence_model_gpu.sh earsm_wj \"Wallin-Johansson EARSM\" 256 512 1000 output/gpu_validation/earsm_wj && \
            ../.github/scripts/test_turbulence_model_gpu.sh earsm_gs \"Gatski-Speziale EARSM\" 256 512 1000 output/gpu_validation/earsm_gs && \
            ../.github/scripts/test_turbulence_model_gpu.sh earsm_pope \"Pope Quadratic EARSM\" 256 512 1000 output/gpu_validation/earsm_pope && \
            echo \"\" && \
            echo \"======================================\" && \
            echo \"6. Periodic Hills - Complex Geometry (Fast Validation)\" && \
            echo \"======================================\" && \
            echo \"   Testing with Baseline model\" && \
            ./periodic_hills --Nx 64 --Ny 48 --nu 0.001 --max_iter 200 --model baseline --num_snapshots 0'" \
          20
    
        echo ""
        echo "==================================================================="
        echo "  CPU/GPU Consistency Validation (Critical)"
        echo "==================================================================="
        echo ""
        ./test_cpu_gpu_consistency
        
        echo ""
        echo "==================================================================="
        echo "  Physics Validation (Comprehensive)"
        echo "==================================================================="
        echo ""
        ./test_physics_validation
        ./test_tg_validation
        
        echo ""
        echo "==================================================================="
        echo "  Perturbed Channel Flow Test"
        echo "==================================================================="
        echo ""
        echo "Testing 2D perturbed channel (validates Poisson solver with non-trivial flow)"
        ./test_poisson_perturbed --Nx 64 --Ny 64 --nu 0.01 --dp_dx -0.01 --max_iter 2000 --tol 1e-6 --verbose false || {
          echo ""
          echo "╔═══════════════════════════════════════════════════════╗"
          echo "║  ✗✗✗ PERTURBED CHANNEL TEST FAILED ✗✗✗              ║"
          echo "╚═══════════════════════════════════════════════════════╝"
          echo ""
          exit 1
        }
        
        echo ""
        echo "✅ Slurm GPU CI completed successfully"
        EOF
        
        sed -i "s|__WORKDIR__|${GITHUB_WORKSPACE}|g" "${SBATCH_SCRIPT}"
        sed -i "s|__SLURM_OUT__|${SLURM_OUT}|g" "${SBATCH_SCRIPT}"
        sed -i "s|__SLURM_ERR__|${SLURM_ERR}|g" "${SBATCH_SCRIPT}"
        
        echo "Submitting Slurm job..."
        sbatch -W "${SBATCH_SCRIPT}"
        
        echo ""
        echo "=== Slurm STDOUT ==="
        cat "${SLURM_OUT}" || true
        echo ""
        echo "=== Slurm STDERR ==="
        cat "${SLURM_ERR}" || true

    - name: Submit GPU performance suite to Slurm (H200, 1 GPU)
      run: |
        set -euo pipefail
        
        SBATCH_SCRIPT="${GITHUB_WORKSPACE}/gpu_ci_perf_${GITHUB_RUN_ID}.sbatch"
        SLURM_OUT="${GITHUB_WORKSPACE}/gpu_ci_perf_${GITHUB_RUN_ID}.out"
        SLURM_ERR="${GITHUB_WORKSPACE}/gpu_ci_perf_${GITHUB_RUN_ID}.err"
        
        cat > "${SBATCH_SCRIPT}" <<'EOF'
        #!/bin/bash
        #SBATCH -J gpu-ci-perf
        #SBATCH -A gts-sbryngelson3
        #SBATCH -N 1
        #SBATCH --ntasks-per-node=4
        #SBATCH -p gpu-h200
        #SBATCH -G 1
        #SBATCH --qos=embers
        #SBATCH -t 00:25:00
        #SBATCH -o __SLURM_OUT__
        #SBATCH -e __SLURM_ERR__
        
        set -euo pipefail
        
        module reset
        module load nvhpc
        
        echo "==================================================================="
        echo "  GPU PERF Slurm Job Environment"
        echo "==================================================================="
        echo ""
        echo "Host: $(hostname)"
        echo "Date: $(date)"
        echo ""
        echo "GPU(s):"
        nvidia-smi
        echo ""
        
        # Hard-require OpenMP target offload (fail if it falls back to CPU)
        export OMP_TARGET_OFFLOAD=MANDATORY
        
        cd "__WORKDIR__"
        chmod +x .github/scripts/*.sh
        
        # Clean rebuild (perf suite must rebuild first)
        rm -rf build_ci_gpu_perf
        mkdir -p build_ci_gpu_perf
        cd build_ci_gpu_perf
        
        echo "=== CMake Configuration (perf) ==="
        CC=nvc CXX=nvc++ cmake .. -DCMAKE_BUILD_TYPE=Release -DUSE_GPU_OFFLOAD=ON 2>&1 | tee cmake_config.log
        echo ""
        echo "=== Building (perf) ==="
        make -j8
        mkdir -p output/gpu_perf
        
        echo ""
        echo "==================================================================="
        echo "  GPU PERF Cases (timed)"
        echo "==================================================================="
        echo ""
        echo "NOTE: Two runs per case (warmup + timed). Output suppressed for stability."
        echo ""
        
        run_case () {
          local name="$1"
          shift 1
          echo "-------------------------------------------"
          echo "CASE: ${name}"
          echo "CMD:  $*"
          echo ""
          # warmup
          "$@" >/dev/null
          # timed
          /usr/bin/time -p "$@" >/dev/null
          echo "PERF_CASE_DONE name=\"${name}\""
          echo ""
        }
        
        # Channel baseline
        run_case "channel_baseline_256x512_2000" \
          ./channel --Nx 256 --Ny 512 --nu 0.001 --max_iter 2000 \
                   --model baseline --dp_dx -0.0001 \
                   --output output/gpu_perf/channel_baseline --num_snapshots 0 --quiet
        
        # Channel SST (transport)
        run_case "channel_sst_256x512_500" \
          ./channel --Nx 256 --Ny 512 --nu 0.001 --max_iter 500 \
                   --model sst --dp_dx -0.0001 \
                   --output output/gpu_perf/channel_sst --num_snapshots 0 --quiet
        
        # Periodic hills (no --quiet flag here; keep interface stable)
        run_case "periodic_hills_baseline_128x96_400" \
          ./periodic_hills --Nx 128 --Ny 96 --nu 0.001 --max_iter 400 \
                          --model baseline --num_snapshots 0
        
        echo "✅ Slurm GPU PERF completed successfully"
        EOF
        
        sed -i "s|__WORKDIR__|${GITHUB_WORKSPACE}|g" "${SBATCH_SCRIPT}"
        sed -i "s|__SLURM_OUT__|${SLURM_OUT}|g" "${SBATCH_SCRIPT}"
        sed -i "s|__SLURM_ERR__|${SLURM_ERR}|g" "${SBATCH_SCRIPT}"
        
        echo "Submitting Slurm perf job..."
        sbatch -W "${SBATCH_SCRIPT}"
        
        echo ""
        echo "=== Slurm PERF STDOUT ==="
        cat "${SLURM_OUT}" || true
        echo ""
        echo "=== Slurm PERF STDERR ==="
        cat "${SLURM_ERR}" || true
    
    - name: Debug info on failure
      if: failure()
      run: |
        echo "=== Debug Information ==="
        echo "Commit: $(git rev-parse HEAD)"
        echo "Branch: $(git rev-parse --abbrev-ref HEAD)"
        echo ""
        echo "=== CMake Config Log ==="
        cat build_ci_gpu_correctness/cmake_config.log 2>/dev/null || echo "No cmake log found (correctness)"
        cat build_ci_cpu_ref/cmake_config.log 2>/dev/null || echo "No cmake log found (cpu_ref)"
        cat build_ci_gpu_perf/cmake_config.log 2>/dev/null || echo "No cmake log found (perf)"
        echo ""
        echo "=== Checking compiled binaries ==="
        ls -lh build_ci_gpu_correctness/test_* 2>/dev/null || echo "No test binaries found (correctness)"
        ls -lh build_ci_cpu_ref/test_* 2>/dev/null || echo "No test binaries found (cpu_ref)"
        ls -lh build_ci_gpu_perf/test_* 2>/dev/null || echo "No test binaries found (perf)"
        echo ""
        echo "=== Verifying source again ==="
        grep -c "k_ptr_" include/solver.hpp src/solver.cpp || echo "k_ptr_ not found!"
    
    - name: Cleanup
      if: always()
      run: |
        rm -f ${GITHUB_WORKSPACE}/gpu_ci_correctness_${GITHUB_RUN_ID}.sbatch || true
        rm -f ${GITHUB_WORKSPACE}/gpu_ci_correctness_${GITHUB_RUN_ID}.out || true
        rm -f ${GITHUB_WORKSPACE}/gpu_ci_correctness_${GITHUB_RUN_ID}.err || true
        rm -f ${GITHUB_WORKSPACE}/gpu_ci_perf_${GITHUB_RUN_ID}.sbatch || true
        rm -f ${GITHUB_WORKSPACE}/gpu_ci_perf_${GITHUB_RUN_ID}.out || true
        rm -f ${GITHUB_WORKSPACE}/gpu_ci_perf_${GITHUB_RUN_ID}.err || true
        rm -rf build_ci_gpu_correctness || true
        rm -rf build_ci_cpu_ref || true
        rm -rf build_ci_gpu_perf || true

