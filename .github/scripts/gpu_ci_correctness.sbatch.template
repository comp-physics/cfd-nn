#!/bin/bash
#SBATCH -J gpu-ci-correctness
#SBATCH -A gts-sbryngelson3
#SBATCH -N 1
#SBATCH --ntasks-per-node=4
#SBATCH -p gpu-h200
#SBATCH -G 1
#SBATCH --qos=embers
#SBATCH -t 00:30:00
#SBATCH -o __SLURM_OUT__
#SBATCH -e __SLURM_ERR__

set -euo pipefail

module reset
module load nvhpc

cd "__WORKDIR__"

echo "==================================================================="
echo "  GPU CI Correctness Suite"
echo "==================================================================="
echo ""
echo "Host: $(hostname)"
echo "Date: $(date)"
echo ""
echo "GPU(s):"
nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv
echo ""

# Set compiler for both CPU and GPU builds
export CC=nvc
export CXX=nvc++

# Clean stale builds but preserve _deps (HYPRE cache from GitHub Actions)
# Note: ci.sh uses build_{cpu,gpu}_hypre directories when HYPRE is enabled
rm -rf build_ci_cpu_sanity
for dir in build_cpu build_gpu build_cpu_hypre build_gpu_hypre; do
    if [ -d "$dir" ]; then
        find "$dir" -mindepth 1 -maxdepth 1 ! -name '_deps' -exec rm -rf {} +
        # Clean hypre-subbuild - contains CMakeCache.txt with absolute paths
        # that break when cache is restored to a different runner path
        rm -rf "$dir/_deps/hypre-subbuild" 2>/dev/null || true
    fi
done
# Check for cached HYPRE builds (CMake will auto-detect and skip rebuild if present)
echo "HYPRE cache status (CMake will detect libHYPRE.a and skip rebuild):"
for dir in build_cpu_hypre build_gpu_hypre; do
    if [ -f "${dir}/_deps/hypre-build/libHYPRE.a" ]; then
        echo "  [CACHED] ${dir}/_deps/hypre-build/libHYPRE.a exists"
        ls -la "${dir}/_deps/" 2>/dev/null | grep hypre || true
    else
        echo "  [MISS] ${dir} - will build HYPRE from source"
    fi
done

# ===================================================================
# Phase 1: CPU Sanity Suite (catches environment-specific issues)
# ===================================================================
echo ""
echo "==================================================================="
echo "  Phase 1: CPU Sanity Suite"
echo "==================================================================="
echo ""
echo "Running CPU tests on H200 node to verify environment correctness."
echo "This catches issues that hosted CI might miss (NVHPC quirks, etc.)"
echo ""

chmod +x .github/scripts/cpu_sanity_suite.sh
.github/scripts/cpu_sanity_suite.sh "${PWD}"

echo ""
echo "[PASS] CPU sanity suite completed - proceeding to GPU build"
echo ""

# ===================================================================
# Phase 2: CPU Reference + GPU Comparison
# ===================================================================

# Hard-require OpenMP target offload (fail if it falls back to CPU)
export OMP_TARGET_OFFLOAD=MANDATORY

# Build CPU reference version (needed for CPU/GPU comparison tests)
echo "Building CPU reference version..."
mkdir -p build_cpu
cd build_cpu
# Force fresh configure: remove CMake cache but preserve _deps (HYPRE)
rm -rf CMakeCache.txt CMakeFiles cmake_install.cmake Makefile lib*.a
cmake .. -DCMAKE_BUILD_TYPE=Release -DUSE_GPU_OFFLOAD=OFF -DBUILD_TESTS=ON
make -j8
cd ..

# Build GPU version
# Note: H200 requires cc90 (Hopper architecture)
echo "Building GPU version..."
mkdir -p build_gpu
cd build_gpu
# Force fresh configure: remove CMake cache but preserve _deps (HYPRE)
rm -rf CMakeCache.txt CMakeFiles cmake_install.cmake Makefile lib*.a
cmake .. -DCMAKE_BUILD_TYPE=Release -DUSE_GPU_OFFLOAD=ON -DBUILD_TESTS=ON -DGPU_CC=90
make -j8
cd ..

# Make scripts executable
chmod +x scripts/ci.sh
chmod +x scripts/check_code_sharing.sh

# Run the full CI test suite (GPU mode is default)
./scripts/ci.sh full

echo ""
echo "[PASS] GPU Correctness Suite completed successfully"
